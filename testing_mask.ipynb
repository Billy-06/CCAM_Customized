{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "from WSOL.utils import *\n",
    "from WSOL.dataset.cub200 import *\n",
    "from WSOL.models.loss import *\n",
    "from WSOL.models.model import *\n",
    "from torchvision import transforms\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "import random\n",
    "import logging\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = '/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM'\n",
    "WSOL_PATH = '/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/WSOL'\n",
    "ROOT_CUB = '/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/data/images/CUB_200_2011/images'\n",
    "ROOT_ILSVRC = '/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/data/images/ILSVRC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark before running\n",
    "cudnn.benchmark = True\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"16\"\n",
    "flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmapy\n",
    "def visualize_heatmap(config, experiments, images, attmaps, cls_name, image_name, phase='train', bboxes=None,\n",
    "                      gt_bboxes=None):\n",
    "    _, c, h, w = images.shape\n",
    "\n",
    "    # Images - The original images\n",
    "    original_images = images.squeeze().to('cpu').detach().numpy()\n",
    "    # Attmaps - The attention maps of the images\n",
    "    attmaps = attmaps.squeeze().to('cpu').detach().numpy()\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "\n",
    "        # create folder\n",
    "        if not os.path.exists('debug/images/{}/{}/colormaps/{}'.format(experiments, phase, cls_name[i])):\n",
    "            os.mkdir('debug/images/{}/{}/colormaps/{}'.format(experiments, phase, cls_name[i]))\n",
    "\n",
    "        attmap = attmaps[i]\n",
    "        attmap = attmap / np.max(attmap)\n",
    "        attmap = np.uint8(attmap * 255)\n",
    "        # colormap = cv2.applyColorMap(cv2.resize(attmap, (w, h)), cv2.COLORMAP_JET)\n",
    "        colormap = cv2.applyColorMap(cv2.resize(attmap, (w, h)), cmapy.cmap('seismic'))\n",
    "\n",
    "        grid = make_grid(images[i].unsqueeze(0), nrow=1, padding=0, pad_value=0,\n",
    "                         normalize=True, range=None)\n",
    "        # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n",
    "        image = grid.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()[..., ::-1]\n",
    "        # print(image.shape, colormap.shape)\n",
    "        cam = colormap + 0.5 * image\n",
    "        cam = cam / np.max(cam)\n",
    "        cam = np.uint8(cam * 255).copy()\n",
    "        bbox_image = image.copy()\n",
    "\n",
    "        if bboxes is not None:\n",
    "            box = bboxes[i][0]\n",
    "\n",
    "            cv2.rectangle(bbox_image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)  # BGR\n",
    "\n",
    "            if gt_bboxes is not None:\n",
    "                if isinstance(gt_bboxes, list):\n",
    "                    for j in range(gt_bboxes[i].shape[0]):\n",
    "                        gtbox = gt_bboxes[i][j]\n",
    "                        cv2.rectangle(bbox_image, (int(gtbox[1]), int(gtbox[2])), (int(gtbox[3]), int(gtbox[4])),\n",
    "                                      (255, 0, 0), 2)\n",
    "                else:\n",
    "                    gtbox = gt_bboxes[i]\n",
    "                    cv2.rectangle(bbox_image, (int(gtbox[1]), int(gtbox[2])), (int(gtbox[3]), int(gtbox[4])),\n",
    "                                  (255, 0, 0),\n",
    "                                  2)\n",
    "\n",
    "        cv2.imwrite(f'debug/images/{experiments}/{phase}/colormaps/{cls_name[i]}/{image_name[i]}_raw.jpg', bbox_image)\n",
    "        cv2.imwrite(f'debug/images/{experiments}/{phase}/colormaps/{cls_name[i]}/{image_name[i]}_heatmap.jpg', cam)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(config, train_loader, model, threshold):\n",
    "\n",
    "    # set up the averagemeters\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    global flag\n",
    "    # record the time\n",
    "    end = time.time()\n",
    "\n",
    "    # extracting\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target, cls_name, img_name) in enumerate(train_loader):\n",
    "\n",
    "            # data to gpu\n",
    "            input = input.cuda()\n",
    "\n",
    "            # inference the model\n",
    "            fg_feats, bg_feats, ccam = model(input)\n",
    "\n",
    "            if flag:\n",
    "                ccam = 1 - ccam\n",
    "\n",
    "            pred_boxes = []  # x0,y0, x1, y1\n",
    "            \n",
    "            for j in range(input.size(0)):\n",
    "                estimated_boxes_at_each_thr, _ = compute_bboxes_from_scoremaps(\n",
    "                    ccam[j, 0, :, :].detach().cpu().numpy().astype(np.float32), [threshold], input.size(-1) / ccam.size(-1),\n",
    "                    multi_contour_eval=False)\n",
    "                pred_boxes.append(estimated_boxes_at_each_thr[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # save predicted bboxes\n",
    "            save_bbox_as_json(config, config.EXPERIMENT, i, 0, pred_boxes, cls_name, img_name)\n",
    "\n",
    "            # print the current testing status\n",
    "            if i % config.PRINT_FREQ == 0:\n",
    "                print('[{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      .format(i, len(train_loader), batch_time=batch_time), flush=True)\n",
    "\n",
    "                visualize_heatmap(config, config.EXPERIMENT, input.clone().detach(), ccam, cls_name, img_name, phase='train', bboxes=pred_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(config, test_loader, model, criterion, epoch):\n",
    "\n",
    "    # set up the averagemeters\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_bg_bg = AverageMeter()\n",
    "    losses_bg_fg = AverageMeter()\n",
    "    losses_fg_fg = AverageMeter()\n",
    "    threshold = [(i + 1) / config.NUM_THRESHOLD for i in range(config.NUM_THRESHOLD - 1)]\n",
    "    print('current threshold list: {}'.format(threshold))\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    global flag\n",
    "    # record the time\n",
    "    end = time.time()\n",
    "\n",
    "    total = 0\n",
    "    Corcorrect = torch.Tensor([[0] for i in range(len(threshold))])\n",
    "\n",
    "    # testing\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target, bboxes, cls_name, img_name) in enumerate(test_loader):\n",
    "\n",
    "            # data to gpu\n",
    "            input = input.cuda()\n",
    "\n",
    "            # inference the model\n",
    "            fg_feats, bg_feats, ccam = model(input)\n",
    "\n",
    "            if flag:\n",
    "                ccam = 1 - ccam\n",
    "\n",
    "            pred_boxes_t = [[] for j in range(len(threshold))]  # x0,y0, x1, y1\n",
    "            for j in range(input.size(0)):\n",
    "\n",
    "                estimated_boxes_at_each_thr, _ = compute_bboxes_from_scoremaps(\n",
    "                    ccam[j, 0, :, :].detach().cpu().numpy().astype(np.float32), threshold, input.size(-1)/ccam.size(-1), multi_contour_eval=False)\n",
    "\n",
    "                for k in range(len(threshold)):\n",
    "                    pred_boxes_t[k].append(estimated_boxes_at_each_thr[k])\n",
    "\n",
    "            loss1 = criterion[0](bg_feats)            # bg contrast bg\n",
    "            loss2 = criterion[1](bg_feats, fg_feats)  # fg contrast fg\n",
    "            loss3 = criterion[2](fg_feats)            # fg contrast fg\n",
    "            loss = loss1 + loss2 + loss3\n",
    "\n",
    "            # acc1 = accuracy(main_out.data, target)[0]\n",
    "            losses.update(loss.data.item(), input.size(0))\n",
    "            losses_bg_bg.update(loss1.data.item(), input.size(0))\n",
    "            losses_bg_fg.update(loss2.data.item(), input.size(0))\n",
    "            losses_fg_fg.update(loss3.data.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            total += input.size(0)\n",
    "            for j in range(len(threshold)):\n",
    "                pred_boxes = pred_boxes_t[j]\n",
    "                pred_boxes = torch.from_numpy(np.array([pred_boxes[k][0] for k in range(len(pred_boxes))])).float()\n",
    "                gt_boxes = bboxes[:, 1:].float()\n",
    "\n",
    "                # calculate\n",
    "                inter = intersect(pred_boxes, gt_boxes)\n",
    "                area_a = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "                area_b = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])\n",
    "                union = area_a + area_b - inter\n",
    "                IOU = inter / union\n",
    "                IOU = torch.where(IOU <= 0.5, IOU, torch.ones(IOU.shape[0]))\n",
    "                IOU = torch.where(IOU > 0.5, IOU, torch.zeros(IOU.shape[0]))\n",
    "\n",
    "                Corcorrect[j] += IOU.sum()\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # print the current testing status\n",
    "            if i % config.PRINT_FREQ == 0:\n",
    "                print('Test: [{0}][{1}/{2}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'BG-C-BG {loss_bgbg.val:.4f} ({loss_bgbg.avg:.4f})\\t'\n",
    "                     'BG-C-FG {loss_bgfg.val:.4f} ({loss_bgfg.avg:.4f})\\t'\n",
    "                     'FG-C-FG {loss_fg_fg.val:.4f} ({loss_fg_fg.avg:.4f})'.format(\n",
    "                    epoch, i, len(test_loader), batch_time=batch_time,\n",
    "                    loss=losses, loss_bgbg=losses_bg_bg, loss_bgfg=losses_bg_fg, loss_fg_fg=losses_fg_fg), flush=True)\n",
    "\n",
    "                # image debug\n",
    "                visualize_heatmap(config, config.EXPERIMENT, input.clone().detach(), ccam, cls_name, img_name, phase='test', bboxes=pred_boxes_t[config.NUM_THRESHOLD // 2], gt_bboxes=bboxes)\n",
    "\n",
    "    current_best_CorLoc = 0\n",
    "    current_best_CorLoc_threshold = 0\n",
    "    for i in range(len(threshold)):\n",
    "        if (Corcorrect[i].item() / total) * 100 > current_best_CorLoc:\n",
    "            current_best_CorLoc = (Corcorrect[i].item() / total) * 100\n",
    "            current_best_CorLoc_threshold = threshold[i]\n",
    "\n",
    "    print('Current => Correct: {:.2f}, threshold: {}'.format(current_best_CorLoc, current_best_CorLoc_threshold))\n",
    "\n",
    "    return current_best_CorLoc, current_best_CorLoc_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(config, train_loader, model, criterion, optimizer, epoch, scheduler):\n",
    "\n",
    "    # set up the averagemeters\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_bg_bg = AverageMeter()\n",
    "    losses_bg_fg = AverageMeter()\n",
    "    losses_fg_fg = AverageMeter()\n",
    "    global flag\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    # record time\n",
    "    end = time.time()\n",
    "\n",
    "    # training step\n",
    "    for i, (input, target, cls_name, img_name) in enumerate(train_loader):\n",
    "\n",
    "        # data to gpu\n",
    "        input = input.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        fg_feats, bg_feats, ccam = model(input)\n",
    "\n",
    "\n",
    "        loss1 = criterion[0](bg_feats)\n",
    "        loss2 = criterion[1](bg_feats, fg_feats)\n",
    "        loss3 = criterion[2](fg_feats)\n",
    "        loss = loss1 + loss2 + loss3\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        losses_bg_bg.update(loss1.data.item(), input.size(0))\n",
    "        losses_bg_fg.update(loss2.data.item(), input.size(0))\n",
    "        losses_fg_fg.update(loss3.data.item(), input.size(0))\n",
    "\n",
    "        if epoch == 0 and i == (len(train_loader)-1):\n",
    "            flag = check_positive(ccam)\n",
    "            print(f\"Is Negative: {flag}\")\n",
    "        if flag:\n",
    "            ccam = 1 - ccam\n",
    "\n",
    "        # measure elapsed time\n",
    "        torch.cuda.synchronize()\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print the current status\n",
    "        if i % config.PRINT_FREQ == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                 'BG-C-BG {loss_bgbg.val:.4f} ({loss_bgbg.avg:.4f})\\t'\n",
    "                 'BG-C-FG {loss_bgfg.val:.4f} ({loss_bgfg.avg:.4f})\\t'\n",
    "                 'FG-C-FG {loss_fg_fg.val:.4f} ({loss_fg_fg.avg:.4f})'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                loss=losses, loss_bgbg=losses_bg_bg, loss_bgfg=losses_bg_fg, loss_fg_fg=losses_fg_fg), flush=True)\n",
    "\n",
    "            # image debug\n",
    "            visualize_heatmap(config, config.EXPERIMENT, input.clone().detach(), ccam, cls_name, img_name)\n",
    "\n",
    "    # print the learning rate\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    print(\"Epoch {:d} finished with lr={:f}\".format(epoch + 1, lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def main(args, config, param_group, train_loader, model, optimizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'ALPHA': 0.05,\n",
      " 'BATCH_SIZE': 16,\n",
      " 'CROP': False,\n",
      " 'CROP_DIR': 'cropped/',\n",
      " 'DATA': 'CUB_200_2011',\n",
      " 'DEBUG': 'debug',\n",
      " 'DEPTH': 50,\n",
      " 'EPOCHS': 20,\n",
      " 'EVALUTATE': False,\n",
      " 'EXPERIMENT': 'CCAM_CUB_IP',\n",
      " 'FDIM': '2048+1024',\n",
      " 'LOG_DIR': 'log/',\n",
      " 'LR': 0.0001,\n",
      " 'MOMENTUM': 0.9,\n",
      " 'NUM_THRESHOLD': 20,\n",
      " 'PRETRAINED': 'supervised',\n",
      " 'PRINT_FREQ': 25,\n",
      " 'RESUME': '',\n",
      " 'ROOT': '/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/data/images/CUB_200_2011/',\n",
      " 'SEED': 1,\n",
      " 'WEIGHT_DECAY': 0.0001,\n",
      " 'WORKERS': 4}\n"
     ]
    }
   ],
   "source": [
    "from WSOL.optimizer import PolyOptimizer\n",
    "\n",
    "\"\"\"\n",
    "OMP_NUM_THREADS=16 \n",
    "CUDA_VISIBLE_DEVICES=0 \n",
    "python train_CCAM_CUB.py \n",
    "--experiment CCAM_CUB_IP \n",
    "--lr 0.0001 \n",
    "--batch_size 16 \n",
    "--pretrained supervised \n",
    "--alpha 0.05\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"train CCAM on CUB dataset\")\n",
    "parser.add_argument('--cfg', help='experiment configuration filename', type=str,\n",
    "                    default='config/CCAM_CUB.yaml')\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--alpha', type=float, default=0.05)\n",
    "# Add a parameter that sets whether the images should be cropped or not\n",
    "parser.add_argument('--crop', type=bool, default=False, help=\"apply image crop or not\")\n",
    "parser.add_argument('--experiment', type=str, required=True, help='record different experiments')\n",
    "parser.add_argument('--pretrained', type=str, required=True, help='adopt different pretrained parameters, [supervised, mocov2, detco]')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# join 'config/CCAM_CUB.yaml' to WSOL_PATH\n",
    "joint_path = os.path.join(WSOL_PATH, 'config/CCAM_CUB.yaml')\n",
    "\n",
    "with open(joint_path, 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    # config = yaml.load(f) #changed to full_load()\n",
    "    # config = yaml.full_load(f)\n",
    "    config = edict(config)\n",
    "    \n",
    "config.EXPERIMENT = \"CCAM_CUB_IP\"\n",
    "config.LR = 0.0001\n",
    "# Add a parameter that sets whether the images should be cropped or not\n",
    "config.CROP = False\n",
    "config.BATCH_SIZE = 16\n",
    "config.PRETRAINED = \"supervised\"\n",
    "config.ALPHA = 0.05\n",
    "\n",
    "# config, args = parse_arg()\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "console = logging.StreamHandler()\n",
    "logging.getLogger('').addHandler(console)\n",
    "logger.info(pprint.pformat(config))\n",
    "\n",
    "if config.SEED != -1:\n",
    "    torch.manual_seed(config.SEED)\n",
    "    torch.cuda.manual_seed(config.SEED)\n",
    "    np.random.seed(config.SEED)\n",
    "    random.seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model...\n",
      "Loading supervised pretrained parameters!\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(\"=> creating log folder...\")\n",
    "# creat_folder(config, args)\n",
    "\n",
    "# log\n",
    "# sys.stdout = Logger('{}/{}_log.txt'.format(config.LOG_DIR, config.EXPERIMENT))\n",
    "\n",
    "# Check if the crop parameter is true then set the crop directory\n",
    "if config.CROP:\n",
    "    print(\"=> using crop augmentation...\")\n",
    "\n",
    "    sys.stdout = Logger('{}/{}/'.format(config.CROP_DIR, config.EXPERIMENT))\n",
    "\n",
    "# create model\n",
    "print(\"=> creating model...\")\n",
    "model = get_model(pretrained=config.PRETRAINED).cuda()\n",
    "param_groups = model.get_parameter_groups()\n",
    "# model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = [SimMaxLoss(metric='cos', alpha=config.ALPHA).cuda(), SimMinLoss(metric='cos').cuda(),\n",
    "                SimMaxLoss(metric='cos', alpha=config.ALPHA).cuda()]\n",
    "\n",
    "# data augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# we follow PSOL to adopt 448x448 as input to generate pseudo bounding boxes\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(480, 480)),\n",
    "    transforms.CenterCrop(size=(448, 448)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 5994 train images!\n",
      "load 5794 test images!\n"
     ]
    }
   ],
   "source": [
    "# wrap to dataset\n",
    "train_data = CUB200(root=config.ROOT, input_size=256, crop_size=224, train=True, transform=train_transforms)\n",
    "test_data = CUB200(root=config.ROOT, input_size=480, crop_size=448, train=False, transform=test_transforms)\n",
    "print('load {} train images!'.format(len(train_data)))\n",
    "print('load {} test images!'.format(len(test_data)))\n",
    "\n",
    "# wrap to dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "    num_workers=config.WORKERS, pin_memory=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "    num_workers=config.WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "max_step = len(train_data) // config.BATCH_SIZE * config.EPOCHS\n",
    "optimizer = PolyOptimizer([\n",
    "    {'params': param_groups[0], 'lr': config.LR, 'weight_decay': config.WEIGHT_DECAY},\n",
    "    {'params': param_groups[1], 'lr': 2 * config.LR, 'weight_decay': 0},\n",
    "    {'params': param_groups[2], 'lr': 10 * config.LR, 'weight_decay': config.WEIGHT_DECAY},\n",
    "    {'params': param_groups[3], 'lr': 20 * config.LR, 'weight_decay': 0}\n",
    "], lr=config.LR, weight_decay=config.WEIGHT_DECAY, max_step=max_step)\n",
    "\n",
    "num_iters = len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_iters * config.EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "global_best_threshold = 0\n",
    "# training part\n",
    "for epoch in range(start_epoch, config.EPOCHS):\n",
    "    # training\n",
    "    train(config, train_loader, model, criterion, optimizer, epoch, scheduler)\n",
    "\n",
    "    # testing\n",
    "    best_CorLoc, best_threshold = test(config, test_loader, model, criterion, epoch)\n",
    "\n",
    "    torch.save(\n",
    "        {\"state_dict\": model.state_dict(),\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"CorLoc\": best_CorLoc,\n",
    "            \"Threshold\": best_threshold,\n",
    "            \"Flag\": flag,\n",
    "            }, '{}/checkpoints/{}/current_epoch.pth'.format(config.DEBUG, config.EXPERIMENT))\n",
    "\n",
    "    global_best_threshold = best_threshold\n",
    "\n",
    "    print('Training finished...')\n",
    "    print('--------------------')\n",
    "\n",
    "    print('Extracting class-agnostic bboxes using best threshold...')\n",
    "    print('--------------------------------------------------------')\n",
    "\n",
    "    \n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(480, 480)),\n",
    "    transforms.CenterCrop(size=(448, 448)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "train_data = CUB200(root=config.ROOT, input_size=480, crop_size=448, train=True, transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "    num_workers=config.WORKERS, pin_memory=False)\n",
    "\n",
    "extract(config, train_loader, model, global_best_threshold)\n",
    "print('Finished.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tsne and matplotlib\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to visualize the features\n",
    "def visualize_features(features, labels, num_classes, title):\n",
    "    '''\n",
    "    Visualize the features using t-SNE.\n",
    "    '''\n",
    "    # Get the color map\n",
    "    cmap = plt.get_cmap('tab20')\n",
    "\n",
    "    # Define the figure\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    axes = plt.axes()\n",
    "\n",
    "    # Get the list of all the classes\n",
    "    classes = [i for i in range(num_classes)]\n",
    "\n",
    "    # Iterate over all the classes\n",
    "    for i, c in enumerate(classes):\n",
    "        # Get the indexes of the images of the current class\n",
    "        indexes = np.where(labels == c)[0]\n",
    "\n",
    "        # Extract the features of the images of the current class\n",
    "        x = [features[index] for index in indexes]\n",
    "\n",
    "        # convert the features to a numpy array\n",
    "        x = np.array(x)\n",
    "\n",
    "        # Apply t-SNE to the features\n",
    "        x_embedded = TSNE(n_components=2).fit_transform(x)\n",
    "\n",
    "        # Scatter plot the points\n",
    "        axes.scatter(x_embedded[:, 0], x_embedded[:, 1], label=c, cmap=cmap)\n",
    "\n",
    "    # Set the title\n",
    "    axes.set_title(title)\n",
    "\n",
    "    # Set the legend\n",
    "    axes.legend(loc='best')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 43.90it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a transform to convert the PIL images to tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "def load_images(root_folder):\n",
    "    # Load all the images in CUB_200_2011/images/ with the labels as the folder names\n",
    "    # and store them in a list of images\n",
    "    cub_images = []\n",
    "    cub_labels = []\n",
    "    \n",
    "    # counter = 0\n",
    "\n",
    "    # Iterate through all the folders in the CUB_200_2011/images/ folder\n",
    "    for folder in tqdm(os.listdir(root_folder)):\n",
    "        # Get the path of the folder\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        # Get the list of all the images in the folder\n",
    "        image_list = os.listdir(folder_path)\n",
    "        # Iterate through all the images in the folder\n",
    "        for img in image_list:\n",
    "            # Get the path of the image\n",
    "            img_path = os.path.join(folder_path, img)\n",
    "            # Load the image and append it to the list of images\n",
    "            # cub_images.append(transform(Image.open(img_path)))\n",
    "            cub_images.append(Image.open(img_path))\n",
    "            # Append the label\n",
    "            cub_labels.append(folder)\n",
    "\n",
    "        # Increment the counter\n",
    "        # counter += 1\n",
    "\n",
    "        # Break if the counter is 100\n",
    "        # if counter == 100:\n",
    "        #     break\n",
    "\n",
    "    return cub_images, cub_labels\n",
    "\n",
    "# Load all the images in CUB_200_2011/images/ with the labels as the folder names\n",
    "# and store them in a list of images\n",
    "cub_images, cub_labels = load_images(ROOT_CUB)\n",
    "\n",
    "# Create a dataset from the list of images and labels\n",
    "dataset = list(zip(cub_images, cub_labels))\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=config.BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torchvision import utils as vutils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(480, 480)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cub_images[0]))\n",
    "print(type(cub_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11788/11788 [00:16<00:00, 712.67it/s] \n"
     ]
    }
   ],
   "source": [
    "from torchvision.datapoints import Image\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# converteach pil image to tensor and store the in a list called cub_images_tensor\n",
    "cub_images_tensor = []\n",
    "for i in tqdm(range(len(cub_images))):\n",
    "    # cub_images_tensor.append(torch.as_tensor(cub_images[i]))\n",
    "    cub_images_tensor.append(Image(cub_images[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(cub_labels)\n",
    "\n",
    "cub_labels_encoded = le.transform(cub_labels)\n",
    "\n",
    "# Convert the labels into a tensor\n",
    "cub_labels_tensor = torch.tensor(cub_labels_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset from the list of images and labels tensors\n",
    "dataset = list(zip(cub_images_tensor, cub_labels_tensor))\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(dataset, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images, labels = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Get the features for the images\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m features \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Get the predictions for the images\u001b[39;00m\n\u001b[1;32m      9\u001b[0m predictions \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(features, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/WSOL/models/model.py:88\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 88\u001b[0m     feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[1;32m     89\u001b[0m     fg_feats, bg_feats, ccam \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mac_head(feats)\n\u001b[1;32m     91\u001b[0m     \u001b[39mreturn\u001b[39;00m fg_feats, bg_feats, ccam\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/WSOL/models/model.py:47\u001b[0m, in \u001b[0;36mResNetSeries.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 47\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     48\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m     49\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# Move the images and labels to the device\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Get the features for the images\n",
    "features = model(images)\n",
    "\n",
    "# Get the predictions for the images\n",
    "predictions = F.softmax(features, dim=1)\n",
    "\n",
    "# Get the class with the highest probability\n",
    "predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = torch.sum(predictions == labels).item() / len(labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualize the features\n",
    "visualize_features(features.cpu().detach().numpy(), labels.cpu().detach().numpy(), 200, 't-SNE visualization of the features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type long int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mTraining Images\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m plt\u001b[39m.\u001b[39mimshow(np\u001b[39m.\u001b[39mtranspose(vutils\u001b[39m.\u001b[39;49mmake_grid(labels[:\u001b[39m64\u001b[39;49m], padding\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mcpu(), (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)))\n\u001b[1;32m     13\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torchvision/utils.py:98\u001b[0m, in \u001b[0;36mmake_grid\u001b[0;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             norm_range(t, value_range)\n\u001b[1;32m     97\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         norm_range(tensor, value_range)\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    101\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtensor should be of type torch.Tensor\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torchvision/utils.py:92\u001b[0m, in \u001b[0;36mmake_grid.<locals>.norm_range\u001b[0;34m(t, value_range)\u001b[0m\n\u001b[1;32m     90\u001b[0m     norm_ip(t, value_range[\u001b[39m0\u001b[39m], value_range[\u001b[39m1\u001b[39m])\n\u001b[1;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     norm_ip(t, \u001b[39mfloat\u001b[39;49m(t\u001b[39m.\u001b[39;49mmin()), \u001b[39mfloat\u001b[39;49m(t\u001b[39m.\u001b[39;49mmax()))\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torchvision/utils.py:85\u001b[0m, in \u001b[0;36mmake_grid.<locals>.norm_ip\u001b[0;34m(img, low, high)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnorm_ip\u001b[39m(img, low, high):\n\u001b[0;32m---> 85\u001b[0m     img\u001b[39m.\u001b[39;49mclamp_(\u001b[39mmin\u001b[39;49m\u001b[39m=\u001b[39;49mlow, \u001b[39mmax\u001b[39;49m\u001b[39m=\u001b[39;49mhigh)\n\u001b[1;32m     86\u001b[0m     img\u001b[39m.\u001b[39msub_(low)\u001b[39m.\u001b[39mdiv_(\u001b[39mmax\u001b[39m(high \u001b[39m-\u001b[39m low, \u001b[39m1e-5\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type long int"
     ]
    }
   ],
   "source": [
    "# resize the tensors to the same width\n",
    "# images_resized = F.interpolate(images, mode='nearest')\n",
    "# labels_resized = F.interpolate(labels, mode='nearest')\n",
    "\n",
    "# Concatenate the images and labels along the width dimension\n",
    "# images_labels = torch.cat((images, labels), dim=3)\n",
    "\n",
    "# Visualize the images\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(labels[:64], padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/5893 [02:34<8:23:58,  5.16s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m features\n\u001b[1;32m     41\u001b[0m \u001b[39m# Extract the features\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m features \u001b[39m=\u001b[39m extract_features(model, images)\n\u001b[1;32m     44\u001b[0m \u001b[39m# visualise the features\u001b[39;00m\n\u001b[1;32m     45\u001b[0m visualize_features(features, labels)\n",
      "Cell \u001b[0;32mIn[22], line 32\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(model, images)\u001b[0m\n\u001b[1;32m     27\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m \u001b[39m# Extract the features\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# The feature size should be (1, 2048, 7, 7)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# inputs, labels = inputs.to(device), labels.to(device)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m feature \u001b[39m=\u001b[39m model(img)\n\u001b[1;32m     34\u001b[0m \u001b[39m# Remove the batch dimension and append the feature to the list\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# The feature size should be (2048, 7, 7)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# features.append(feature.squeeze(0))\u001b[39;00m\n\u001b[1;32m     37\u001b[0m features\u001b[39m.\u001b[39mappend(feature)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/WSOL/models/model.py:88\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 88\u001b[0m     feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[1;32m     89\u001b[0m     fg_feats, bg_feats, ccam \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mac_head(feats)\n\u001b[1;32m     91\u001b[0m     \u001b[39mreturn\u001b[39;00m fg_feats, bg_feats, ccam\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/WSOL/models/model.py:52\u001b[0m, in \u001b[0;36mResNetSeries.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     50\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[0;32m---> 52\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m     53\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m     54\u001b[0m x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/WSOL/models/resnet.py:108\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m    The Forward Pass\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m    ---------------- \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    - The result is returned. \u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m residual \u001b[39m=\u001b[39m x\n\u001b[0;32m--> 108\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    109\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m    110\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def extract_features(model, images):\n",
    "    '''\n",
    "    Extract the features of the images using the pretrained ResNet50 model.\n",
    "    The features are extracted from the last convolutional layer.\n",
    "    '''\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Define a list to store the features\n",
    "    features = []\n",
    "\n",
    "    # Iterate over all the images\n",
    "    for img in tqdm(images):\n",
    "        # Convert the image to a torch.Tensor and normalize the image\n",
    "        # The image size should be (3, 224, 224)\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])(img)\n",
    "        \n",
    "        # Add a batch dimension to the image\n",
    "        # The image size should be (1, 3, 224, 224)\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "        \n",
    "        # Move the image to the device\n",
    "        img = img.to(device)\n",
    "\n",
    "        # Extract the features\n",
    "        # The feature size should be (1, 2048, 7, 7)\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        feature = model(img)\n",
    "\n",
    "        # Remove the batch dimension and append the feature to the list\n",
    "        # The feature size should be (2048, 7, 7)\n",
    "        # features.append(feature.squeeze(0))\n",
    "        features.append(feature)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Extract the features\n",
    "features = extract_features(model, images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cuda cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# visualise the features\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m visualize_features(features, labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# visualise the features\n",
    "visualize_features(features, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
